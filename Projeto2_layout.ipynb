{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Luiz Felipe Domingues Valente\n",
    "\n",
    "Nome: João Guilherme Cintra de Freitas Almeida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import re \n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: @jofenina"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Dados de autenticação do twitter:\n",
    "\n",
    "Coloque aqui o identificador da conta no twitter: @jofenina\n",
    "\n",
    "leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Produto escolhido:\n",
    "produto = 'snickers'\n",
    "\n",
    "Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "    msgs.append(msg.full_text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tr_sujos = pd.read_excel(\"snickers.xlsx\", sheet_name = \"Treinamento\")\n",
    "tweets_tr_sujos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento = tweets_tr_sujos.loc[:, \"Treinamento\"]\n",
    "rel=tweets_tr_sujos.loc[:, \"Relevância\"]\n",
    "treinamento.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Criando função de limpeza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \n",
    "#  Limpeza de caracteres\n",
    "\n",
    "    punctuation = '[!\\-.:?;@#$%&*_1234567890]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpando tweets \"Treinamento\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_limpos1 = []\n",
    "t_limpos2 = []\n",
    "t_limpos = []\n",
    "for linha in treinamento:\n",
    "    b=linha.lower()\n",
    "    t_limpos1.append(cleanup(b))\n",
    "for e in t_limpos1:\n",
    "    a = e.strip()\n",
    "    t_limpos2.append(a)\n",
    "for s in t_limpos2:\n",
    "    z = ' '.join(s.split())\n",
    "    t_limpos.append(z)\n",
    "t_limpos[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Espaça emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search your emoji\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI\n",
    "\n",
    "# add space near your emoji\n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "results=[add_space(text) for text in t_limpos]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_palavras = []\n",
    "for palavra in results:\n",
    "    lista_palavras += palavra.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequência de Palavras nos Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(lista_palavras)\n",
    "p_palavras = serie.value_counts(True)\n",
    "p_palavras.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabela com limpeza e relevância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tr = pd.DataFrame({\"Treinamento\": results,\"Relevância\": rel})\n",
    "t_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Relevantes Limpos e frequência de suas Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rel= t_tr[(t_tr.Relevância==1)]\n",
    "t_rel.head()\n",
    "len(t_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_tweets = ' '.join(t_rel.loc[:,\"Treinamento\"])\n",
    "a = pd.Series(serie_tweets.split())\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_tweets_relativar = a.value_counts()\n",
    "tabela_tweets_relativar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_palavra_relevante = tabela_tweets_relativar / len(lista_palavras)\n",
    "# prob_palavra_relevante.head()\n",
    "# prob_palavra_irrelevante = tabela_tweets_relativa_n_rel / len(lista_palavras)\n",
    "# prob_palavra_irrelevante.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets não Relevantes Limpos e frequência de suas Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_n_rel= t_tr[(t_tr.Relevância==0)]\n",
    "t_n_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_tweets_n_rel = ' '.join(t_n_rel.loc[:,\"Treinamento\"])\n",
    "a_n_rel = pd.Series(serie_tweets_n_rel.split())\n",
    "a_n_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencia que a palavra aparece dentro dos irrelevantes\n",
    "tabela_tweets_relativa_n_rel = a_n_rel.value_counts()\n",
    "tabela_tweets_relativa_n_rel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Função Classificadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificador(tweet):\n",
    "    palavras=tweet.split()\n",
    "    prob_r=1\n",
    "    prob_nr=1\n",
    "    g = tabela_tweets_relativar.sum()+len(lista_palavras)\n",
    "    z = tabela_tweets_relativa_n_rel.sum()+len(lista_palavras)\n",
    "    for p in palavras:\n",
    "        if p in tabela_tweets_relativar:\n",
    "            prob_r *= (tabela_tweets_relativar[p]+1) / g\n",
    "      \n",
    "        if p in tabela_tweets_relativa_n_rel:\n",
    "            prob_nr *= (tabela_tweets_relativa_n_rel[p]+1) / z\n",
    "        else:\n",
    "            prob_nr *= 1 / z\n",
    "            prob_r *= 1 / g\n",
    "    if prob_r > prob_nr:\n",
    "        return \"É relevante\"\n",
    "    else: \n",
    "        return \"Não é relevante\"\n",
    "print(classificador(\"eu amo snickers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando os testes e limpando\n",
    "tweets_tt_sujos = pd.read_excel(\"snickers.xlsx\", sheet_name = \"Teste\")\n",
    "teste = tweets_tt_sujos.loc[:, \"Teste\"]\n",
    "relt=tweets_tt_sujos.loc[:, \"Relevância\"]\n",
    "tt_limpos1 = []\n",
    "tt_limpos2 = []\n",
    "tt_limpos = []\n",
    "for linhat in teste:\n",
    "    bt = linhat.lower()\n",
    "    tt_limpos1.append(cleanup(bt))\n",
    "for et in tt_limpos1:\n",
    "    a = et.strip()\n",
    "    tt_limpos2.append(a)\n",
    "for st in tt_limpos2:\n",
    "    z = ' '.join(st.split())\n",
    "    tt_limpos.append(z)\n",
    "tt_limpos[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI\n",
    "\n",
    "# add space near your emoji\n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "resultst=[add_space(text) for text in tt_limpos]\n",
    "lista_palavrast = []\n",
    "for pepe in resultst:\n",
    "    lista_palavrast += pepe.split()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_te = pd.Series(resultst)\n",
    "dic_te={}\n",
    "for frase in serie_te:\n",
    "    resultado = classificador(frase)\n",
    "    dic_te[frase]=resultado\n",
    "ir = 0\n",
    "inr = 0\n",
    "for k,v in dic.items():\n",
    "    if v == 'É relevante':\n",
    "        ir +=  1\n",
    "    elif v == 'Não é relevante':\n",
    "        inr += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "lista2 = []\n",
    "verdadeiro_positivo = 0\n",
    "verdadeiro_negativo = 0\n",
    "falso_positivo = 0\n",
    "falso_negativo = 0\n",
    "for k,v in dic.items():\n",
    "    lista.append(v)\n",
    "\n",
    "for e in relt:\n",
    "    lista2.append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista3=[]\n",
    "for i in lista:\n",
    "    if i==\"É relevante\":\n",
    "        lista3.append(1)\n",
    "    if i==\"Não é relevante\":\n",
    "        lista3.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "verdadeiro_positivo = 0\n",
    "verdadeiro_negativo = 0\n",
    "falso_positivo = 0\n",
    "falso_negativo = 0\n",
    "for k in o:\n",
    "    if k == lista2[i] == 1:\n",
    "        verdadeiro_positivo += 1\n",
    "    if k == lista2[i] == 0:\n",
    "        verdadeiro_negativo += 1\n",
    "    if k != lista2[i] and lista2[i] == 0:\n",
    "        falso_positivo += 1\n",
    "    if k != lista2[i] and lista2[i] == 1:\n",
    "        falso_negativo += 1\n",
    "    i += 1\n",
    "total = falso_positivo+falso_negativo+verdadeiro_positivo+verdadeiro_negativo\n",
    "        \n",
    "print(verdadeiro_negativo)\n",
    "print(verdadeiro_positivo)\n",
    "print(falso_negativo)\n",
    "print(falso_positivo)\n",
    "print(falso_positivo+falso_negativo+verdadeiro_positivo+verdadeiro_negativo)\n",
    "print('Precisão de acerto dos relevantes segundo nosso código: {}%'.format(((verdadeiro_positivo + falso_negativo)/total)*100))\n",
    "print('Precisão de acerto dos irrelevantes nosso código: {}%'.format(((verdadeiro_negativo + falso_positivo)/total)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusão dados:\n",
    "A análise dos tweets evidenciou que nosso classificador não é muito preciso para classificar tweets sobre o Snickers, que é um chocolate da empresa Mars Wrigley. Nosso classificador teve uma acurácia de aproximadamente 44% de precisão de acerto para tweets relevantes e precisão de aproximadamente 55% de precisão para tweets irrelevantes. Embora a precisão não tenha sido suficiente para usar o algoritmo de forma confiável, ele é um classificador Naive-Bayes, que tem uma base correta e que pode ser refinado e alterado para se obter melhor acurácia.\n",
    "\n",
    "Possíveis alterações ou incrementações para aumentar a precisão do algoritmo seria: aumentar a base de dados, se pegarmos o dobro, ou o triplo de tweets, é possível que o maior número de palavras para treino aumente a acurácia, também poderia se criar uma função que identifica frases sarcásticas, de tal modo que não classifique certas frases como relevantes assim deixando o classificador mais eficiente. \n",
    "\n",
    "O classificador Naive-Bayes pode ser utilizado em outras aplicações, como para analisar a caixa de e-mails de uma pessoa, e distinguindo se certos e-mails vão para a caixa de spam, ou se vão para a caixa principal. Em geral esse método pode ser utilizado para  para otimizar e melhorar softwares ou aparelhos, com o intuito de aumentar a facilidade e a simplicidade de se usar uma ferramenta, assim diminuindo erros e aumentando a produtividade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
